{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\iNeuron_Projects\\\\End_to_End_ML_Dental_Implant_Sandblasting\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\iNeuron_Projects\\\\End_to_End_ML_Dental_Implant_Sandblasting'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-20 00:44:49,774: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-07-20 00:44:49,821: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-07-20 00:44:49,834: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2024-07-20 00:44:49,837: INFO: common: created directory at: artifacts]\n",
      "[2024-07-20 00:44:49,840: INFO: 2977693376: Config: {'artifacts_root': 'artifacts', 'data_ingestion': {'root_dir': 'artifacts/data_ingestion', 'source_URL': 'https://github.com/farshidhesami/Branching-tutorial/raw/master/Sandblasting-Condition.zip', 'local_data_file': 'artifacts/data_ingestion/data.zip', 'unzip_dir': 'artifacts/data_ingestion'}, 'data_validation': {'root_dir': 'artifacts/data_validation', 'unzip_data_dir': 'artifacts/data_ingestion/Sandblasting-Condition.csv', 'STATUS_FILE': 'artifacts/data_validation/status.txt'}, 'data_transformation': {'root_dir': 'artifacts/data_transformation', 'data_path': 'artifacts/data_ingestion/Sandblasting-Condition.csv', 'transformed_train_path': 'artifacts/data_transformation/train.csv', 'transformed_test_path': 'artifacts/data_transformation/test.csv'}, 'model_trainer': {'root_dir': 'artifacts/model_trainer', 'train_data_path': 'artifacts/data_transformation/train.csv', 'test_data_path': 'artifacts/data_transformation/test.csv', 'model_name': 'model.joblib', 'poly_features_path': 'artifacts/model_trainer/poly_features.joblib', 'model_path': 'artifacts/model_trainer/models'}, 'model_evaluation': {'root_dir': 'artifacts/model_evaluation', 'test_data_path': 'artifacts/data_transformation/test.csv', 'model_path': 'artifacts/model_trainer/models', 'metric_file_name': 'artifacts/model_evaluation/metrics.json', 'target_column': 'Result (1=Passed, 0=Failed)'}, 'model_saving': {'model_path': 'artifacts/model_trainer/models', 'poly_features_path': 'artifacts/model_trainer/poly_features.joblib'}}]\n",
      "[2024-07-20 00:44:49,843: INFO: 2977693376: Params: {'data_preprocessing': {'imputation_strategy': 'mean'}, 'data_transformation': {'polynomial_features_degree': 2, 'scaling_method': 'StandardScaler', 'test_size': 0.2, 'random_state': 42}, 'model_training': {'target_column': 'Result (1=Passed, 0=Failed)', 'models': {'ridge': {'alpha': 1.0}, 'elasticnet': {'alpha': 1.0, 'l1_ratio': 0.5}, 'bayesian_ridge': {}, 'huber_regressor': {'alpha': 0.0001, 'epsilon': 1.35, 'max_iter': 2000, 'tol': 1e-05, 'warm_start': False}, 'random_forest': {'n_estimators': 100, 'max_depth': 20, 'random_state': 42}, 'gradient_boosting': {'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 3}, 'svr': {'C': 1.0, 'epsilon': 0.1}, 'xgboost': {'objective': 'reg:squarederror', 'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 5, 'subsample': 1.0}}}, 'hyperparameter_tuning': {'cv': 5, 'scoring': 'neg_mean_absolute_error', 'ridge': {'param_grid': {'alpha': [0.1, 1.0, 10.0, 100.0]}}, 'elasticnet': {'param_grid': {'alpha': [0.1, 1.0, 10.0], 'l1_ratio': [0.1, 0.5, 0.9]}}, 'huber_regressor': {'param_grid': {'alpha': [0.0001, 0.001, 0.01], 'epsilon': [1.35, 1.5, 1.75], 'max_iter': [1000, 2000], 'tol': [1e-05, 0.0001]}}, 'svr': {'param_grid': {'C': [0.1, 1.0, 10.0], 'epsilon': [0.01, 0.1, 1.0], 'kernel': ['linear', 'rbf']}}, 'random_forest': {'param_grid': {'n_estimators': [50, 100, 200], 'max_depth': [10, 20, 'None'], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4]}}, 'gradient_boosting': {'param_grid': {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 0.2], 'max_depth': [3, 5, 7]}}, 'xgboost': {'param_grid': {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 0.2], 'max_depth': [3, 5, 7]}}}}]\n",
      "[2024-07-20 00:44:49,845: INFO: 2977693376: Schema: {'COLUMNS': {'Angle of Sandblasting': 'int64', 'Pressure of Sandblasting (bar)': 'int64', 'Temperture of Acid Etching': 'int64', 'Time of Acid Etching (min)': 'int64', 'Voltage of Anodizing (v)': 'int64', 'Time of  Anodizing (min)': 'int64'}, 'TARGET_COLUMNS': {'(Sa) Average of Surface roughness (micrometer)': 'float64', 'Cell Viability (%)': 'int64', 'Result (1=Passed, 0=Failed)': 'int64'}}]\n",
      "[2024-07-20 00:44:49,861: INFO: 2977693376: Test data loaded successfully.]\n",
      "[2024-07-20 00:44:49,865: ERROR: 2977693376: Model file not found: artifacts\\model_trainer\\models\\best_model_sa.joblib]\n",
      "[2024-07-20 00:44:49,870: ERROR: 2977693376: Error loading models or polynomial features]\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Farshid Hesami\\AppData\\Local\\Temp\\ipykernel_5104\\2977693376.py\", line 82, in load_model\n",
      "    raise FileNotFoundError(f\"No such file: '{model_sa_path}'\")\n",
      "FileNotFoundError: No such file: 'artifacts\\model_trainer\\models\\best_model_sa.joblib'\n",
      "[2024-07-20 00:44:49,873: ERROR: 2977693376: No such file: 'artifacts\\model_trainer\\models\\best_model_sa.joblib']\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Farshid Hesami\\AppData\\Local\\Temp\\ipykernel_5104\\2977693376.py\", line 350, in <module>\n",
      "    model_evaluator.load_model()\n",
      "  File \"C:\\Users\\Farshid Hesami\\AppData\\Local\\Temp\\ipykernel_5104\\2977693376.py\", line 98, in load_model\n",
      "    raise e\n",
      "  File \"C:\\Users\\Farshid Hesami\\AppData\\Local\\Temp\\ipykernel_5104\\2977693376.py\", line 82, in load_model\n",
      "    raise FileNotFoundError(f\"No such file: '{model_sa_path}'\")\n",
      "FileNotFoundError: No such file: 'artifacts\\model_trainer\\models\\best_model_sa.joblib'\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "No such file: 'artifacts\\model_trainer\\models\\best_model_sa.joblib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 357\u001b[0m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    356\u001b[0m     logger\u001b[38;5;241m.\u001b[39mexception(e)\n\u001b[1;32m--> 357\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Cell \u001b[1;32mIn[5], line 350\u001b[0m\n\u001b[0;32m    348\u001b[0m model_evaluator \u001b[38;5;241m=\u001b[39m ModelEvaluation(config\u001b[38;5;241m=\u001b[39mmodel_evaluation_config)\n\u001b[0;32m    349\u001b[0m model_evaluator\u001b[38;5;241m.\u001b[39mload_data()\n\u001b[1;32m--> 350\u001b[0m \u001b[43mmodel_evaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    351\u001b[0m model_evaluator\u001b[38;5;241m.\u001b[39mdata_exploration()\n\u001b[0;32m    352\u001b[0m model_evaluator\u001b[38;5;241m.\u001b[39mevaluate_model()\n",
      "Cell \u001b[1;32mIn[5], line 98\u001b[0m, in \u001b[0;36mModelEvaluation.load_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     97\u001b[0m     logger\u001b[38;5;241m.\u001b[39mexception(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError loading models or polynomial features\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 98\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Cell \u001b[1;32mIn[5], line 82\u001b[0m, in \u001b[0;36mModelEvaluation.load_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_sa_path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m     81\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel file not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_sa_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 82\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo such file: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_sa_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_cv_path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m     85\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel file not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_cv_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: No such file: 'artifacts\\model_trainer\\models\\best_model_sa.joblib'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from Dental_Implant_Sandblasting.constants import CONFIG_FILE_PATH, PARAMS_FILE_PATH, SCHEMA_FILE_PATH\n",
    "from Dental_Implant_Sandblasting.utils.common import read_yaml, create_directories, save_json\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error, median_absolute_error\n",
    "from urllib.parse import urlparse\n",
    "import joblib\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from sklearn.model_selection import learning_curve, cross_val_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "import yaml\n",
    "import logging\n",
    "\n",
    "# Initialize logger\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Define ModelEvaluationConfig dataclass\n",
    "@dataclass(frozen=True)\n",
    "class ModelEvaluationConfig:\n",
    "    root_dir: Path\n",
    "    test_data_path: Path\n",
    "    model_path: Path\n",
    "    all_params: dict\n",
    "    metric_file_name: Path\n",
    "    target_column: str\n",
    "\n",
    "# Define ConfigurationManager class\n",
    "class ConfigurationManager:\n",
    "    def __init__(self, config_filepath=CONFIG_FILE_PATH, params_filepath=PARAMS_FILE_PATH, schema_filepath=SCHEMA_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "        create_directories([self.config['artifacts_root']])\n",
    "        logger.info(f\"Config: {self.config}\")\n",
    "        logger.info(f\"Params: {self.params}\")\n",
    "        logger.info(f\"Schema: {self.schema}\")\n",
    "\n",
    "    def get_model_evaluation_config(self) -> ModelEvaluationConfig:\n",
    "        evaluation_config = self.config['model_evaluation']\n",
    "        return ModelEvaluationConfig(\n",
    "            root_dir=Path(evaluation_config['root_dir']),\n",
    "            test_data_path=Path(evaluation_config['test_data_path']),\n",
    "            model_path=Path(evaluation_config['model_path']),\n",
    "            all_params=self.config,\n",
    "            metric_file_name=Path(evaluation_config['metric_file_name']),\n",
    "            target_column=evaluation_config['target_column']\n",
    "        )\n",
    "\n",
    "# Define ModelEvaluation class\n",
    "class ModelEvaluation:\n",
    "    def __init__(self, config: ModelEvaluationConfig):\n",
    "        self.config = config\n",
    "        self.test_data = None\n",
    "        self.model_sa = None\n",
    "        self.model_cv = None\n",
    "        self.poly = None\n",
    "\n",
    "    def load_data(self):\n",
    "        try:\n",
    "            self.test_data = pd.read_csv(self.config.test_data_path)\n",
    "            logger.info(\"Test data loaded successfully.\")\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Error loading test data\")\n",
    "            raise e\n",
    "\n",
    "    def load_model(self):\n",
    "        try:\n",
    "            model_sa_path = self.config.model_path / 'best_model_sa.joblib'\n",
    "            model_cv_path = self.config.model_path / 'best_model_cv.joblib'\n",
    "            poly_path = self.config.model_path / 'poly_features.joblib'\n",
    "            \n",
    "            if not model_sa_path.exists():\n",
    "                logger.error(f\"Model file not found: {model_sa_path}\")\n",
    "                raise FileNotFoundError(f\"No such file: '{model_sa_path}'\")\n",
    "            \n",
    "            if not model_cv_path.exists():\n",
    "                logger.error(f\"Model file not found: {model_cv_path}\")\n",
    "                raise FileNotFoundError(f\"No such file: '{model_cv_path}'\")\n",
    "            \n",
    "            if not poly_path.exists():\n",
    "                logger.error(f\"Polynomial features file not found: {poly_path}\")\n",
    "                raise FileNotFoundError(f\"No such file: '{poly_path}'\")\n",
    "            \n",
    "            self.model_sa = joblib.load(model_sa_path)\n",
    "            self.model_cv = joblib.load(model_cv_path)\n",
    "            self.poly = joblib.load(poly_path)\n",
    "            logger.info(\"Models and polynomial features loaded successfully.\")\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Error loading models or polynomial features\")\n",
    "            raise e\n",
    "\n",
    "    def data_exploration(self):\n",
    "        try:\n",
    "            logger.info(\"Performing data exploration.\")\n",
    "            print(\"First few rows of the dataset:\")\n",
    "            display(self.test_data.head())\n",
    "\n",
    "            print(f\"\\nData shape: {self.test_data.shape}\")\n",
    "            print(\"\\nData info:\")\n",
    "            self.test_data.info()\n",
    "\n",
    "            print(\"\\nData types:\")\n",
    "            print(self.test_data.dtypes)\n",
    "\n",
    "            print(\"\\nSummary statistics:\")\n",
    "            display(self.test_data.describe(include='all'))\n",
    "\n",
    "            print(\"\\nMissing values by column:\")\n",
    "            print(self.test_data.isnull().sum())\n",
    "\n",
    "            # Further EDA plots\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.heatmap(self.test_data.corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "            plt.title(\"Correlation Heatmap\")\n",
    "            plt.show()\n",
    "\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            sns.histplot(self.test_data['(Sa) Average of Surface roughness (micrometer)'], kde=True, bins=20)\n",
    "            plt.title(\"Distribution of Surface Roughness (Sa)\")\n",
    "            plt.xlabel(\"Surface Roughness (Sa) (µm)\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.show()\n",
    "\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            sns.histplot(self.test_data['Cell Viability (%)'], kde=True, bins=20)\n",
    "            plt.title(\"Distribution of Cell Viability\")\n",
    "            plt.xlabel(\"Cell Viability (%)\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Error during data exploration\")\n",
    "            raise e\n",
    "\n",
    "    def eval_metrics(self, actual, pred):\n",
    "        rmse = np.sqrt(mean_squared_error(actual, pred))\n",
    "        mae = mean_absolute_error(actual, pred)\n",
    "        r2 = r2_score(actual, pred)\n",
    "        mape = mean_absolute_percentage_error(actual, pred)\n",
    "        medae = median_absolute_error(actual, pred)\n",
    "        return rmse, mae, r2, mape, medae\n",
    "\n",
    "    def save_results(self):\n",
    "        try:\n",
    "            test_data = pd.read_csv(self.config.test_data_path)\n",
    "            model = joblib.load(self.config.model_path / 'model.joblib')\n",
    "\n",
    "            test_x = test_data.drop([self.config.target_column], axis=1)\n",
    "            test_y = test_data[[self.config.target_column]]\n",
    "\n",
    "            predicted_qualities = model.predict(test_x)\n",
    "\n",
    "            rmse, mae, r2, mape, medae = self.eval_metrics(test_y, predicted_qualities)\n",
    "\n",
    "            # Saving metrics as local\n",
    "            scores = {\"rmse\": rmse, \"mae\": mae, \"r2\": r2, \"mape\": mape, \"medae\": medae}\n",
    "            save_json(path=Path(self.config.metric_file_name), data=scores)\n",
    "            logger.info(\"Results saved successfully.\")\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Error during saving results\")\n",
    "            raise e\n",
    "\n",
    "    def evaluate_model(self):\n",
    "        try:\n",
    "            logger.info(\"Evaluating the model.\")\n",
    "            X_test = self.test_data.drop(columns=['(Sa) Average of Surface roughness (micrometer)', 'Cell Viability (%)', 'Result (1=Passed, 0=Failed)'])\n",
    "            y_sa_test = self.test_data['(Sa) Average of Surface roughness (micrometer)']\n",
    "            y_cv_test = self.test_data['Cell Viability (%)']\n",
    "\n",
    "            X_poly_test = self.poly.transform(X_test)\n",
    "            y_sa_pred = self.model_sa.predict(X_poly_test)\n",
    "            valid_indices = (y_sa_pred > 1.5) & (y_sa_pred < 2.5)\n",
    "            y_cv_pred = np.zeros_like(y_cv_test)\n",
    "            if any(valid_indices):\n",
    "                y_cv_pred[valid_indices] = self.model_cv.predict(X_poly_test[valid_indices])\n",
    "\n",
    "            # Evaluation metrics for Surface Roughness (Sa)\n",
    "            mae_sa = mean_absolute_error(y_sa_test, y_sa_pred)\n",
    "            rmse_sa = np.sqrt(mean_squared_error(y_sa_test, y_sa_pred))\n",
    "            r2_sa = r2_score(y_sa_test, y_sa_pred)\n",
    "            mape_sa = mean_absolute_percentage_error(y_sa_test, y_sa_pred)\n",
    "            medae_sa = median_absolute_error(y_sa_test, y_sa_pred)\n",
    "\n",
    "            print(f\"Surface Roughness (Sa) - Test MAE: {mae_sa:.4f}\")\n",
    "            print(f\"Surface Roughness (Sa) - Test RMSE: {rmse_sa:.4f}\")\n",
    "            print(f\"Surface Roughness (Sa) - Test R2: {r2_sa:.4f}\")\n",
    "            print(f\"Surface Roughness (Sa) - Test MAPE: {mape_sa:.4f}\")\n",
    "            print(f\"Surface Roughness (Sa) - Test MedAE: {medae_sa:.4f}\")\n",
    "\n",
    "            # Cross-validation scores\n",
    "            cv_scores = cross_val_score(self.model_sa, X_poly_test, y_sa_test, cv=5, scoring='neg_mean_absolute_error')\n",
    "            print(f\"Cross-Validation MAE (Sa): {-cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "\n",
    "            # Residuals\n",
    "            residuals = y_sa_test - y_sa_pred\n",
    "\n",
    "            # Residual plot\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.scatter(y_sa_pred, residuals, alpha=0.5)\n",
    "            plt.hlines(y=0, xmin=min(y_sa_pred), xmax=max(y_sa_pred), color='r', linestyles='dashed')\n",
    "            plt.xlabel('Predicted Surface Roughness (Sa)')\n",
    "            plt.ylabel('Residuals')\n",
    "            plt.title('Residual Plot for Surface Roughness (Sa)')\n",
    "            plt.show()\n",
    "\n",
    "            # Distribution of Residuals\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.histplot(residuals, kde=True)\n",
    "            plt.title('Distribution of Residuals for Surface Roughness (Sa)')\n",
    "            plt.xlabel('Residuals')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.show()\n",
    "\n",
    "            # Evaluate Cell Viability (CV)\n",
    "            if any(valid_indices):\n",
    "                mae_cv = mean_absolute_error(y_cv_test[valid_indices], y_cv_pred[valid_indices])\n",
    "                rmse_cv = np.sqrt(mean_squared_error(y_cv_test[valid_indices], y_cv_pred[valid_indices]))\n",
    "                r2_cv = r2_score(y_cv_test[valid_indices], y_cv_pred[valid_indices])\n",
    "                mape_cv = mean_absolute_percentage_error(y_cv_test[valid_indices], y_cv_pred[valid_indices])\n",
    "                medae_cv = median_absolute_error(y_cv_test[valid_indices], y_cv_pred[valid_indices])\n",
    "\n",
    "                print(f\"Cell Viability (CV) - Test MAE: {mae_cv:.4f}\")\n",
    "                print(f\"Cell Viability (CV) - Test RMSE: {rmse_cv:.4f}\")\n",
    "                print(f\"Cell Viability (CV) - Test R2: {r2_cv:.4f}\")\n",
    "                print(f\"Cell Viability (CV) - Test MAPE: {mape_cv:.4f}\")\n",
    "                print(f\"Cell Viability (CV) - Test MedAE: {medae_cv:.4f}\")\n",
    "\n",
    "                valid_colors = np.where(y_cv_pred[valid_indices] > 90, 'green', 'red')\n",
    "\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                plt.scatter(y_cv_test[valid_indices], y_cv_pred[valid_indices], alpha=0.5, c=valid_colors)\n",
    "                plt.plot([min(y_cv_test[valid_indices]), max(y_cv_test[valid_indices])], [min(y_cv_test[valid_indices]), max(y_cv_test[valid_indices])], color='r')\n",
    "                plt.xlabel('Actual Cell Viability')\n",
    "                plt.ylabel('Predicted Cell Viability')\n",
    "                plt.title('Actual vs Predicted Cell Viability (Valid Predictions Only)')\n",
    "                plt.show()\n",
    "\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.scatter(y_sa_test, y_sa_pred, alpha=0.5)\n",
    "            plt.plot([min(y_sa_test), max(y_sa_test)], [min(y_sa_test), max(y_sa_test)], color='r')\n",
    "            plt.xlabel('Actual Surface Roughness (Sa)')\n",
    "            plt.ylabel('Predicted Surface Roughness (Sa)')\n",
    "            plt.title('Actual vs Predicted Surface Roughness (Sa)')\n",
    "            plt.show()\n",
    "\n",
    "            # Learning Curves\n",
    "            train_sizes, train_scores, test_scores = learning_curve(self.model_sa, X_poly_test, y_sa_test, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10))\n",
    "            train_scores_mean = -train_scores.mean(axis=1)\n",
    "            test_scores_mean = -test_scores.mean(axis=1)\n",
    "\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(train_sizes, train_scores_mean, label='Training Error')\n",
    "            plt.plot(train_sizes, test_scores_mean, label='Validation Error')\n",
    "            plt.title('Learning Curve')\n",
    "            plt.xlabel('Training Set Size')\n",
    "            plt.ylabel('Mean Absolute Error')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Error during model evaluation\")\n",
    "            raise e\n",
    "\n",
    "    def make_predictions(self):\n",
    "        try:\n",
    "            logger.info(\"Making predictions.\")\n",
    "            X_new = self.test_data.drop(columns=['(Sa) Average of Surface roughness (micrometer)', 'Cell Viability (%)', 'Result (1=Passed, 0=Failed)'])\n",
    "            X_poly_new = self.poly.transform(X_new)\n",
    "            y_sa_pred_new = self.model_sa.predict(X_poly_new)\n",
    "            y_cv_pred_new = np.zeros_like(y_sa_pred_new)\n",
    "            valid_indices_new = (y_sa_pred_new > 1.5) & (y_sa_pred_new < 2.5)\n",
    "            if any(valid_indices_new):\n",
    "                y_cv_pred_new[valid_indices_new] = self.model_cv.predict(X_poly_new[valid_indices_new])\n",
    "\n",
    "            results = pd.DataFrame({\n",
    "                'Predicted Surface Roughness (Sa)': y_sa_pred_new,\n",
    "                'Predicted Cell Viability (%)': y_cv_pred_new,\n",
    "                'Validity': np.where(y_cv_pred_new > 90, 'green', 'red')\n",
    "            })\n",
    "            print(results)\n",
    "\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.scatter(y_sa_pred_new, y_cv_pred_new, c=results['Validity'], alpha=0.5)\n",
    "            plt.axhline(90, color='r', linestyle='dashed', linewidth=1)\n",
    "            plt.xlabel('Predicted Surface Roughness (Sa)')\n",
    "            plt.ylabel('Predicted Cell Viability (%)')\n",
    "            plt.title('Predicted Surface Roughness vs Predicted Cell Viability')\n",
    "            plt.colorbar(label='Validity')\n",
    "            plt.show()\n",
    "\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.hist(y_sa_pred_new, bins=20, alpha=0.7, label='Surface Roughness (Sa)')\n",
    "            plt.axvline(1.5, color='r', linestyle='dashed', linewidth=1)\n",
    "            plt.axvline(2.5, color='r', linestyle='dashed', linewidth=1)\n",
    "            plt.title('Distribution of Predicted Surface Roughness (Sa)')\n",
    "            plt.xlabel('Surface Roughness (Sa)')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.hist(y_cv_pred_new, bins=20, alpha=0.7, label='Cell Viability (%)', color='orange')\n",
    "            plt.axvline(90, color='r', linestyle='dashed', linewidth=1)\n",
    "            plt.title('Distribution of Predicted Cell Viability (%)')\n",
    "            plt.xlabel('Cell Viability (%)')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "            validity_counts = results['Validity'].value_counts()\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            plt.bar(validity_counts.index, validity_counts.values, color=['red', 'green'])\n",
    "            plt.xlabel('Prediction Validity')\n",
    "            plt.ylabel('Count')\n",
    "            plt.title('Count of Valid vs Invalid Predictions')\n",
    "            plt.show()\n",
    "\n",
    "            residuals = y_sa_pred_new - y_sa_pred_new\n",
    "\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.scatter(y_sa_pred_new, residuals, alpha=0.5)\n",
    "            plt.hlines(y=0, xmin=min(y_sa_pred_new), xmax=max(y_sa_pred_new), color='r', linestyles='dashed')\n",
    "            plt.xlabel('Predicted Surface Roughness (Sa)')\n",
    "            plt.ylabel('Residuals')\n",
    "            plt.title('Residual Plot for Predictions')\n",
    "            plt.show()\n",
    "\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.histplot(residuals, kde=True, color='blue')\n",
    "            plt.title('Distribution of Residuals for Predictions')\n",
    "            plt.xlabel('Residuals')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Error during predictions\")\n",
    "            raise e\n",
    "\n",
    "# Pipeline execution\n",
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_evaluation_config = config.get_model_evaluation_config()\n",
    "    model_evaluator = ModelEvaluation(config=model_evaluation_config)\n",
    "    model_evaluator.load_data()\n",
    "    model_evaluator.load_model()\n",
    "    model_evaluator.data_exploration()\n",
    "    model_evaluator.evaluate_model()\n",
    "    model_evaluator.make_predictions()\n",
    "    model_evaluator.save_results()\n",
    "except Exception as e:\n",
    "    logger.exception(e)\n",
    "    raise e\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
