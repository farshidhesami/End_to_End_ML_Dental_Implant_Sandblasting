{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\iNeuron_Projects\\\\End_to_End_ML_Dental_Implant_Sandblasting\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\iNeuron_Projects\\\\End_to_End_ML_Dental_Implant_Sandblasting'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-17 01:47:39,503: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-07-17 01:47:39,547: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-07-17 01:47:39,559: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2024-07-17 01:47:39,563: INFO: common: created directory at: artifacts]\n",
      "[2024-07-17 01:47:39,579: INFO: 542098582: Test data loaded successfully.]\n",
      "[2024-07-17 01:47:39,582: ERROR: 542098582: Error loading models or polynomial features]\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Farshid Hesami\\AppData\\Local\\Temp\\ipykernel_12456\\542098582.py\", line 71, in load_model\n",
      "    self.model_sa = joblib.load(self.config.model_path / 'best_model_sa.joblib')\n",
      "  File \"c:\\Users\\Farshid Hesami\\anaconda3\\envs\\mlProject\\lib\\site-packages\\joblib\\numpy_pickle.py\", line 650, in load\n",
      "    with open(filename, 'rb') as f:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'artifacts\\\\model_trainer\\\\model.joblib\\\\best_model_sa.joblib'\n",
      "[2024-07-17 01:47:39,593: ERROR: 542098582: [Errno 2] No such file or directory: 'artifacts\\\\model_trainer\\\\model.joblib\\\\best_model_sa.joblib']\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Farshid Hesami\\AppData\\Local\\Temp\\ipykernel_12456\\542098582.py\", line 329, in <module>\n",
      "    model_evaluator.load_model()\n",
      "  File \"C:\\Users\\Farshid Hesami\\AppData\\Local\\Temp\\ipykernel_12456\\542098582.py\", line 77, in load_model\n",
      "    raise e\n",
      "  File \"C:\\Users\\Farshid Hesami\\AppData\\Local\\Temp\\ipykernel_12456\\542098582.py\", line 71, in load_model\n",
      "    self.model_sa = joblib.load(self.config.model_path / 'best_model_sa.joblib')\n",
      "  File \"c:\\Users\\Farshid Hesami\\anaconda3\\envs\\mlProject\\lib\\site-packages\\joblib\\numpy_pickle.py\", line 650, in load\n",
      "    with open(filename, 'rb') as f:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'artifacts\\\\model_trainer\\\\model.joblib\\\\best_model_sa.joblib'\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'artifacts\\\\model_trainer\\\\model.joblib\\\\best_model_sa.joblib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 336\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    335\u001b[0m     logger\u001b[38;5;241m.\u001b[39mexception(e)\n\u001b[1;32m--> 336\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Cell \u001b[1;32mIn[5], line 329\u001b[0m\n\u001b[0;32m    327\u001b[0m model_evaluator \u001b[38;5;241m=\u001b[39m ModelEvaluation(config\u001b[38;5;241m=\u001b[39mmodel_evaluation_config)\n\u001b[0;32m    328\u001b[0m model_evaluator\u001b[38;5;241m.\u001b[39mload_data()\n\u001b[1;32m--> 329\u001b[0m \u001b[43mmodel_evaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    330\u001b[0m model_evaluator\u001b[38;5;241m.\u001b[39mdata_exploration()\n\u001b[0;32m    331\u001b[0m model_evaluator\u001b[38;5;241m.\u001b[39mevaluate_model()\n",
      "Cell \u001b[1;32mIn[5], line 77\u001b[0m, in \u001b[0;36mModelEvaluation.load_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     76\u001b[0m     logger\u001b[38;5;241m.\u001b[39mexception(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError loading models or polynomial features\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 77\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Cell \u001b[1;32mIn[5], line 71\u001b[0m, in \u001b[0;36mModelEvaluation.load_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 71\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_sa \u001b[38;5;241m=\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbest_model_sa.joblib\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_cv \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmodel_path \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model_cv.joblib\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     73\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoly \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmodel_path \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpoly_features.joblib\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Farshid Hesami\\anaconda3\\envs\\mlProject\\lib\\site-packages\\joblib\\numpy_pickle.py:650\u001b[0m, in \u001b[0;36mload\u001b[1;34m(filename, mmap_mode)\u001b[0m\n\u001b[0;32m    648\u001b[0m         obj \u001b[38;5;241m=\u001b[39m _unpickle(fobj)\n\u001b[0;32m    649\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 650\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    651\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _read_fileobject(f, filename, mmap_mode) \u001b[38;5;28;01mas\u001b[39;00m fobj:\n\u001b[0;32m    652\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    653\u001b[0m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[0;32m    654\u001b[0m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[0;32m    655\u001b[0m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'artifacts\\\\model_trainer\\\\model.joblib\\\\best_model_sa.joblib'"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error, median_absolute_error\n",
    "from sklearn.model_selection import learning_curve, cross_val_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from dataclasses import dataclass\n",
    "import yaml\n",
    "import joblib\n",
    "import logging\n",
    "from Dental_Implant_Sandblasting.utils.common import read_yaml, create_directories\n",
    "from Dental_Implant_Sandblasting.constants import CONFIG_FILE_PATH, PARAMS_FILE_PATH, SCHEMA_FILE_PATH\n",
    "\n",
    "# Initialize logger\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Define ModelEvaluationConfig dataclass\n",
    "@dataclass(frozen=True)\n",
    "class ModelEvaluationConfig:\n",
    "    root_dir: Path\n",
    "    test_data_path: Path\n",
    "    model_path: Path\n",
    "    all_params: dict\n",
    "    metric_file_name: Path\n",
    "    target_column: str\n",
    "\n",
    "# Define ConfigurationManager class\n",
    "class ConfigurationManager:\n",
    "    def __init__(self, config_filepath=CONFIG_FILE_PATH, params_filepath=PARAMS_FILE_PATH, schema_filepath=SCHEMA_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "        create_directories([self.config['artifacts_root']])\n",
    "\n",
    "    def get_model_evaluation_config(self) -> ModelEvaluationConfig:\n",
    "        evaluation_config = self.config['model_evaluation']\n",
    "        return ModelEvaluationConfig(\n",
    "            root_dir=Path(evaluation_config['root_dir']),\n",
    "            test_data_path=Path(evaluation_config['test_data_path']),\n",
    "            model_path=Path(evaluation_config['model_path']),\n",
    "            all_params=self.config,\n",
    "            metric_file_name=Path(evaluation_config['metric_file_name']),\n",
    "            target_column=evaluation_config['target_column']\n",
    "        )\n",
    "\n",
    "# Define ModelEvaluation class\n",
    "class ModelEvaluation:\n",
    "    def __init__(self, config: ModelEvaluationConfig):\n",
    "        self.config = config\n",
    "        self.test_data = None\n",
    "        self.model_sa = None\n",
    "        self.model_cv = None\n",
    "        self.poly = None\n",
    "\n",
    "    def load_data(self):\n",
    "        try:\n",
    "            self.test_data = pd.read_csv(self.config.test_data_path)\n",
    "            logger.info(\"Test data loaded successfully.\")\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Error loading test data\")\n",
    "            raise e\n",
    "\n",
    "    def load_model(self):\n",
    "        try:\n",
    "            self.model_sa = joblib.load(self.config.model_path / 'best_model_sa.joblib')\n",
    "            self.model_cv = joblib.load(self.config.model_path / 'best_model_cv.joblib')\n",
    "            self.poly = joblib.load(self.config.model_path / 'poly_features.joblib')\n",
    "            logger.info(\"Models and polynomial features loaded successfully.\")\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Error loading models or polynomial features\")\n",
    "            raise e\n",
    "\n",
    "    def data_exploration(self):\n",
    "        try:\n",
    "            logger.info(\"Performing data exploration.\")\n",
    "            print(\"First few rows of the dataset:\")\n",
    "            display(self.test_data.head())\n",
    "\n",
    "            print(f\"\\nData shape: {self.test_data.shape}\")\n",
    "            print(\"\\nData info:\")\n",
    "            self.test_data.info()\n",
    "\n",
    "            print(\"\\nData types:\")\n",
    "            print(self.test_data.dtypes)\n",
    "\n",
    "            print(\"\\nSummary statistics:\")\n",
    "            display(self.test_data.describe(include='all'))\n",
    "\n",
    "            print(\"\\nMissing values by column:\")\n",
    "            print(self.test_data.isnull().sum())\n",
    "\n",
    "            # Further EDA plots\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.heatmap(self.test_data.corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "            plt.title(\"Correlation Heatmap\")\n",
    "            plt.show()\n",
    "\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            sns.histplot(self.test_data['(Sa) Average of Surface roughness (micrometer)'], kde=True, bins=20)\n",
    "            plt.title(\"Distribution of Surface Roughness (Sa)\")\n",
    "            plt.xlabel(\"Surface Roughness (Sa) (µm)\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.show()\n",
    "\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            sns.histplot(self.test_data['Cell Viability (%)'], kde=True, bins=20)\n",
    "            plt.title(\"Distribution of Cell Viability\")\n",
    "            plt.xlabel(\"Cell Viability (%)\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Error during data exploration\")\n",
    "            raise e\n",
    "\n",
    "    def eval_metrics(self, actual, pred):\n",
    "        rmse = np.sqrt(mean_squared_error(actual, pred))\n",
    "        mae = mean_absolute_error(actual, pred)\n",
    "        r2 = r2_score(actual, pred)\n",
    "        mape = mean_absolute_percentage_error(actual, pred)\n",
    "        medae = median_absolute_error(actual, pred)\n",
    "        return rmse, mae, r2, mape, medae\n",
    "\n",
    "    def save_results(self):\n",
    "        try:\n",
    "            test_data = pd.read_csv(self.config.test_data_path)\n",
    "            model = joblib.load(self.config.model_path / 'model.joblib')\n",
    "\n",
    "            test_x = test_data.drop([self.config.target_column], axis=1)\n",
    "            test_y = test_data[[self.config.target_column]]\n",
    "\n",
    "            predicted_qualities = model.predict(test_x)\n",
    "\n",
    "            rmse, mae, r2, mape, medae = self.eval_metrics(test_y, predicted_qualities)\n",
    "\n",
    "            # Saving metrics as local\n",
    "            scores = {\"rmse\": rmse, \"mae\": mae, \"r2\": r2, \"mape\": mape, \"medae\": medae}\n",
    "            save_json(path=Path(self.config.metric_file_name), data=scores)\n",
    "            logger.info(\"Results saved successfully.\")\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Error during saving results\")\n",
    "            raise e\n",
    "\n",
    "    def evaluate_model(self):\n",
    "        try:\n",
    "            logger.info(\"Evaluating the model.\")\n",
    "            X_test = self.test_data.drop(columns=['(Sa) Average of Surface roughness (micrometer)', 'Cell Viability (%)', 'Result (1=Passed, 0=Failed)'])\n",
    "            y_sa_test = self.test_data['(Sa) Average of Surface roughness (micrometer)']\n",
    "            y_cv_test = self.test_data['Cell Viability (%)']\n",
    "\n",
    "            X_poly_test = self.poly.transform(X_test)\n",
    "            y_sa_pred = self.model_sa.predict(X_poly_test)\n",
    "            valid_indices = (y_sa_pred > 1.5) & (y_sa_pred < 2.5)\n",
    "            y_cv_pred = np.zeros_like(y_cv_test)\n",
    "            if any(valid_indices):\n",
    "                y_cv_pred[valid_indices] = self.model_cv.predict(X_poly_test[valid_indices])\n",
    "\n",
    "            # Evaluation metrics for Surface Roughness (Sa)\n",
    "            mae_sa = mean_absolute_error(y_sa_test, y_sa_pred)\n",
    "            rmse_sa = np.sqrt(mean_squared_error(y_sa_test, y_sa_pred))\n",
    "            r2_sa = r2_score(y_sa_test, y_sa_pred)\n",
    "            mape_sa = mean_absolute_percentage_error(y_sa_test, y_sa_pred)\n",
    "            medae_sa = median_absolute_error(y_sa_test, y_sa_pred)\n",
    "\n",
    "            print(f\"Surface Roughness (Sa) - Test MAE: {mae_sa:.4f}\")\n",
    "            print(f\"Surface Roughness (Sa) - Test RMSE: {rmse_sa:.4f}\")\n",
    "            print(f\"Surface Roughness (Sa) - Test R2: {r2_sa:.4f}\")\n",
    "            print(f\"Surface Roughness (Sa) - Test MAPE: {mape_sa:.4f}\")\n",
    "            print(f\"Surface Roughness (Sa) - Test MedAE: {medae_sa:.4f}\")\n",
    "\n",
    "            # Cross-validation scores\n",
    "            cv_scores = cross_val_score(self.model_sa, X_poly_test, y_sa_test, cv=5, scoring='neg_mean_absolute_error')\n",
    "            print(f\"Cross-Validation MAE (Sa): {-cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "\n",
    "            # Residuals\n",
    "            residuals = y_sa_test - y_sa_pred\n",
    "\n",
    "            # Residual plot\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.scatter(y_sa_pred, residuals, alpha=0.5)\n",
    "            plt.hlines(y=0, xmin=min(y_sa_pred), xmax=max(y_sa_pred), color='r', linestyles='dashed')\n",
    "            plt.xlabel('Predicted Surface Roughness (Sa)')\n",
    "            plt.ylabel('Residuals')\n",
    "            plt.title('Residual Plot for Surface Roughness (Sa)')\n",
    "            plt.show()\n",
    "\n",
    "            # Distribution of Residuals\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.histplot(residuals, kde=True)\n",
    "            plt.title('Distribution of Residuals for Surface Roughness (Sa)')\n",
    "            plt.xlabel('Residuals')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.show()\n",
    "\n",
    "            # Evaluate Cell Viability (CV)\n",
    "            if any(valid_indices):\n",
    "                mae_cv = mean_absolute_error(y_cv_test[valid_indices], y_cv_pred[valid_indices])\n",
    "                rmse_cv = np.sqrt(mean_squared_error(y_cv_test[valid_indices], y_cv_pred[valid_indices]))\n",
    "                r2_cv = r2_score(y_cv_test[valid_indices], y_cv_pred[valid_indices])\n",
    "                mape_cv = mean_absolute_percentage_error(y_cv_test[valid_indices], y_cv_pred[valid_indices])\n",
    "                medae_cv = median_absolute_error(y_cv_test[valid_indices], y_cv_pred[valid_indices])\n",
    "\n",
    "                print(f\"Cell Viability (CV) - Test MAE: {mae_cv:.4f}\")\n",
    "                print(f\"Cell Viability (CV) - Test RMSE: {rmse_cv:.4f}\")\n",
    "                print(f\"Cell Viability (CV) - Test R2: {r2_cv:.4f}\")\n",
    "                print(f\"Cell Viability (CV) - Test MAPE: {mape_cv:.4f}\")\n",
    "                print(f\"Cell Viability (CV) - Test MedAE: {medae_cv:.4f}\")\n",
    "\n",
    "                valid_colors = np.where(y_cv_pred[valid_indices] > 90, 'green', 'red')\n",
    "\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                plt.scatter(y_cv_test[valid_indices], y_cv_pred[valid_indices], alpha=0.5, c=valid_colors)\n",
    "                plt.plot([min(y_cv_test[valid_indices]), max(y_cv_test[valid_indices])], [min(y_cv_test[valid_indices]), max(y_cv_test[valid_indices])], color='r')\n",
    "                plt.xlabel('Actual Cell Viability')\n",
    "                plt.ylabel('Predicted Cell Viability')\n",
    "                plt.title('Actual vs Predicted Cell Viability (Valid Predictions Only)')\n",
    "                plt.show()\n",
    "\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.scatter(y_sa_test, y_sa_pred, alpha=0.5)\n",
    "            plt.plot([min(y_sa_test), max(y_sa_test)], [min(y_sa_test), max(y_sa_test)], color='r')\n",
    "            plt.xlabel('Actual Surface Roughness (Sa)')\n",
    "            plt.ylabel('Predicted Surface Roughness (Sa)')\n",
    "            plt.title('Actual vs Predicted Surface Roughness (Sa)')\n",
    "            plt.show()\n",
    "\n",
    "            # Learning Curves\n",
    "            train_sizes, train_scores, test_scores = learning_curve(self.model_sa, X_poly_test, y_sa_test, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10))\n",
    "            train_scores_mean = -train_scores.mean(axis=1)\n",
    "            test_scores_mean = -test_scores.mean(axis=1)\n",
    "\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(train_sizes, train_scores_mean, label='Training Error')\n",
    "            plt.plot(train_sizes, test_scores_mean, label='Validation Error')\n",
    "            plt.title('Learning Curve')\n",
    "            plt.xlabel('Training Set Size')\n",
    "            plt.ylabel('Mean Absolute Error')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Error during model evaluation\")\n",
    "            raise e\n",
    "\n",
    "    def make_predictions(self):\n",
    "        try:\n",
    "            logger.info(\"Making predictions.\")\n",
    "            X_new = self.test_data.drop(columns=['(Sa) Average of Surface roughness (micrometer)', 'Cell Viability (%)', 'Result (1=Passed, 0=Failed)'])\n",
    "            X_poly_new = self.poly.transform(X_new)\n",
    "            y_sa_pred_new = self.model_sa.predict(X_poly_new)\n",
    "            y_cv_pred_new = np.zeros_like(y_sa_pred_new)\n",
    "            valid_indices_new = (y_sa_pred_new > 1.5) & (y_sa_pred_new < 2.5)\n",
    "            if any(valid_indices_new):\n",
    "                y_cv_pred_new[valid_indices_new] = self.model_cv.predict(X_poly_new[valid_indices_new])\n",
    "\n",
    "            results = pd.DataFrame({\n",
    "                'Predicted Surface Roughness (Sa)': y_sa_pred_new,\n",
    "                'Predicted Cell Viability (%)': y_cv_pred_new,\n",
    "                'Validity': np.where(y_cv_pred_new > 90, 'green', 'red')\n",
    "            })\n",
    "            print(results)\n",
    "\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.scatter(y_sa_pred_new, y_cv_pred_new, c=results['Validity'], alpha=0.5)\n",
    "            plt.axhline(90, color='r', linestyle='dashed', linewidth=1)\n",
    "            plt.xlabel('Predicted Surface Roughness (Sa)')\n",
    "            plt.ylabel('Predicted Cell Viability (%)')\n",
    "            plt.title('Predicted Surface Roughness vs Predicted Cell Viability')\n",
    "            plt.colorbar(label='Validity')\n",
    "            plt.show()\n",
    "\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.hist(y_sa_pred_new, bins=20, alpha=0.7, label='Surface Roughness (Sa)')\n",
    "            plt.axvline(1.5, color='r', linestyle='dashed', linewidth=1)\n",
    "            plt.axvline(2.5, color='r', linestyle='dashed', linewidth=1)\n",
    "            plt.title('Distribution of Predicted Surface Roughness (Sa)')\n",
    "            plt.xlabel('Surface Roughness (Sa)')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.hist(y_cv_pred_new, bins=20, alpha=0.7, label='Cell Viability (%)', color='orange')\n",
    "            plt.axvline(90, color='r', linestyle='dashed', linewidth=1)\n",
    "            plt.title('Distribution of Predicted Cell Viability (%)')\n",
    "            plt.xlabel('Cell Viability (%)')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "            validity_counts = results['Validity'].value_counts()\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            plt.bar(validity_counts.index, validity_counts.values, color=['red', 'green'])\n",
    "            plt.xlabel('Prediction Validity')\n",
    "            plt.ylabel('Count')\n",
    "            plt.title('Count of Valid vs Invalid Predictions')\n",
    "            plt.show()\n",
    "\n",
    "            residuals = y_sa_pred_new - y_sa_pred_new\n",
    "\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.scatter(y_sa_pred_new, residuals, alpha=0.5)\n",
    "            plt.hlines(y=0, xmin=min(y_sa_pred_new), xmax=max(y_sa_pred_new), color='r', linestyles='dashed')\n",
    "            plt.xlabel('Predicted Surface Roughness (Sa)')\n",
    "            plt.ylabel('Residuals')\n",
    "            plt.title('Residual Plot for Predictions')\n",
    "            plt.show()\n",
    "\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.histplot(residuals, kde=True, color='blue')\n",
    "            plt.title('Distribution of Residuals for Predictions')\n",
    "            plt.xlabel('Residuals')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Error during predictions\")\n",
    "            raise e\n",
    "\n",
    "# Pipeline execution\n",
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_evaluation_config = config.get_model_evaluation_config()\n",
    "    model_evaluator = ModelEvaluation(config=model_evaluation_config)\n",
    "    model_evaluator.load_data()\n",
    "    model_evaluator.load_model()\n",
    "    model_evaluator.data_exploration()\n",
    "    model_evaluator.evaluate_model()\n",
    "    model_evaluator.make_predictions()\n",
    "    model_evaluator.save_results()\n",
    "except Exception as e:\n",
    "    logger.exception(e)\n",
    "    raise e\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
