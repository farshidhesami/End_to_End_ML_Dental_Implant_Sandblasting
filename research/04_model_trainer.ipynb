{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\iNeuron_Projects\\\\End_to_End_ML_Dental_Implant_Sandblasting\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\iNeuron_Projects\\\\End_to_End_ML_Dental_Implant_Sandblasting'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from Dental_Implant_Sandblasting import logger\n",
    "from Dental_Implant_Sandblasting.utils.common import read_yaml, create_directories\n",
    "from Dental_Implant_Sandblasting.constants import CONFIG_FILE_PATH, PARAMS_FILE_PATH, SCHEMA_FILE_PATH\n",
    "from sklearn.linear_model import Ridge, ElasticNet, BayesianRidge, HuberRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "\n",
    "# Define ModelTrainerConfig dataclass\n",
    "@dataclass(frozen=True)\n",
    "class ModelTrainerConfig:\n",
    "    root_dir: Path\n",
    "    train_data_path: Path\n",
    "    test_data_path: Path\n",
    "    test_size: float\n",
    "    random_state: int\n",
    "    models: dict\n",
    "    param_grids: dict\n",
    "    alpha: float\n",
    "    l1_ratio: float\n",
    "    target_column: str\n",
    "\n",
    "# Define ConfigurationManager class\n",
    "class ConfigurationManager:\n",
    "    def __init__(self, config_filepath=CONFIG_FILE_PATH, params_filepath=PARAMS_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_model_trainer_config(self) -> ModelTrainerConfig:\n",
    "        config = self.config.model_trainer\n",
    "        params = self.params.model_training\n",
    "        param_grids = self.params.hyperparameter_tuning\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "        model_trainer_config = ModelTrainerConfig(\n",
    "            root_dir=Path(config.root_dir),\n",
    "            train_data_path=Path(config.train_data_path),\n",
    "            test_data_path=Path(config.test_data_path),\n",
    "            test_size=params['test_size'],\n",
    "            random_state=params['random_state'],\n",
    "            models=params['models'],\n",
    "            param_grids=param_grids,\n",
    "            alpha=params['elasticnet']['alpha'],\n",
    "            l1_ratio=params['elasticnet']['l1_ratio'],\n",
    "            target_column=self.params.target_column\n",
    "        )\n",
    "        return model_trainer_config\n",
    "\n",
    "# Define ModelTrainer class\n",
    "class ModelTrainer:\n",
    "    def __init__(self, config: ModelTrainerConfig):\n",
    "        self.config = config\n",
    "        self.models = {\n",
    "            \"Ridge\": Ridge(),\n",
    "            \"ElasticNet\": ElasticNet(alpha=self.config.alpha, l1_ratio=self.config.l1_ratio),\n",
    "            \"BayesianRidge\": BayesianRidge(),\n",
    "            \"HuberRegressor\": HuberRegressor(),\n",
    "            \"RandomForest\": RandomForestRegressor(random_state=self.config.random_state),\n",
    "            \"GradientBoosting\": GradientBoostingRegressor(random_state=self.config.random_state),\n",
    "            \"SVR\": SVR(),\n",
    "            \"XGBoost\": XGBRegressor(random_state=self.config.random_state)\n",
    "        }\n",
    "        self.param_grids = self.config.param_grids\n",
    "\n",
    "    def load_data(self):\n",
    "        train_data = pd.read_csv(self.config.train_data_path)\n",
    "        test_data = pd.read_csv(self.config.test_data_path)\n",
    "\n",
    "        X_train = train_data.drop(columns=[self.config.target_column])\n",
    "        y_train = train_data[self.config.target_column]\n",
    "\n",
    "        X_test = test_data.drop(columns=[self.config.target_column])\n",
    "        y_test = test_data[self.config.target_column]\n",
    "\n",
    "        return X_train, y_train, X_test, y_test\n",
    "\n",
    "    def evaluate_models(self, X_train, y_train):\n",
    "        model_performance = {}\n",
    "\n",
    "        for model_name, model in self.models.items():\n",
    "            print(f\"Training {model_name}...\")\n",
    "            cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_absolute_error')\n",
    "            mae = -cv_scores.mean()\n",
    "\n",
    "            model_performance[model_name] = {\n",
    "                \"MAE\": mae\n",
    "            }\n",
    "\n",
    "            print(f\"{model_name} - MAE: {mae:.4f}\")\n",
    "\n",
    "        return model_performance\n",
    "\n",
    "    def hyperparameter_tuning(self, X_train, y_train):\n",
    "        best_models = {}\n",
    "\n",
    "        for model_name in [\"RandomForest\", \"GradientBoosting\", \"Ridge\", \"ElasticNet\", \"HuberRegressor\", \"SVR\", \"XGBoost\"]:\n",
    "            grid_search = GridSearchCV(\n",
    "                estimator=self.models[model_name],\n",
    "                param_grid=self.param_grids[model_name.lower()],\n",
    "                cv=5,\n",
    "                scoring='neg_mean_absolute_error',\n",
    "                n_jobs=-1,\n",
    "                verbose=2\n",
    "            )\n",
    "            print(f\"Tuning {model_name}...\")\n",
    "            grid_search.fit(X_train, y_train)\n",
    "            best_models[model_name] = grid_search.best_estimator_\n",
    "            print(f\"Best parameters for {model_name}: {grid_search.best_params_}\")\n",
    "\n",
    "        return best_models\n",
    "\n",
    "    def evaluate_best_models(self, best_models, X_test, y_test):\n",
    "        performance_metrics = {}\n",
    "\n",
    "        for model_name, model in best_models.items():\n",
    "            y_pred = model.predict(X_test)\n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "            rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "            performance_metrics[model_name] = {\n",
    "                \"MAE\": mae,\n",
    "                \"RMSE\": rmse,\n",
    "                \"R2\": r2\n",
    "            }\n",
    "\n",
    "            print(f\"{model_name} - Test MAE: {mae:.4f}\")\n",
    "            print(f\"{model_name} - Test RMSE: {rmse:.4f}\")\n",
    "            print(f\"{model_name} - Test R2: {r2:.4f}\")\n",
    "\n",
    "        return performance_metrics\n",
    "\n",
    "    def save_best_models(self, best_models):\n",
    "        for model_name, model in best_models.items():\n",
    "            joblib.dump(model, self.config.root_dir / f\"{model_name}.joblib\")\n",
    "            logger.info(f\"Saved best model {model_name} to {self.config.root_dir / f'{model_name}.joblib'}\")\n",
    "\n",
    "    def execute(self):\n",
    "        X_train, y_train, X_test, y_test = self.load_data()\n",
    "        model_performance = self.evaluate_models(X_train, y_train)\n",
    "        performance_df = pd.DataFrame(model_performance).T\n",
    "        print(\"\\nModel Performance:\\n\", performance_df)\n",
    "\n",
    "        best_models = self.hyperparameter_tuning(X_train, y_train)\n",
    "        performance_metrics = self.evaluate_best_models(best_models, X_test, y_test)\n",
    "\n",
    "        best_hyperparameters = {model_name: model.get_params() for model_name, model in best_models.items()}\n",
    "        print(\"\\nBest Hyperparameters:\\n\", best_hyperparameters)\n",
    "        print(\"\\nPerformance Metrics:\\n\", performance_metrics)\n",
    "\n",
    "        self.save_best_models(best_models)\n",
    "\n",
    "        # Visualizations\n",
    "        metrics_df = pd.DataFrame(performance_metrics).T\n",
    "\n",
    "        plt.figure(figsize=(14, 6))\n",
    "        plt.subplot(1, 3, 1)\n",
    "        sns.barplot(x=metrics_df.index, y=[metrics_df['MAE'][model] for model in metrics_df.index])\n",
    "        plt.title('MAE for Best Models')\n",
    "        plt.ylabel('Mean Absolute Error')\n",
    "        plt.xlabel('Model')\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        sns.barplot(x=metrics_df.index, y=[metrics_df['RMSE'][model] for model in metrics_df.index])\n",
    "        plt.title('RMSE for Best Models')\n",
    "        plt.ylabel('Root Mean Squared Error')\n",
    "        plt.xlabel('Model')\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        sns.barplot(x=metrics_df.index, y=[metrics_df['R2'][model] for model in metrics_df.index])\n",
    "        plt.title('R2 for Best Models')\n",
    "        plt.ylabel('R-Squared')\n",
    "        plt.xlabel('Model')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Pipeline execution\n",
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_trainer_config = config.get_model_trainer_config()\n",
    "    model_trainer = ModelTrainer(config=model_trainer_config)\n",
    "    model_trainer.execute()\n",
    "except Exception as e:\n",
    "    logger.exception(e)\n",
    "    raise e\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
