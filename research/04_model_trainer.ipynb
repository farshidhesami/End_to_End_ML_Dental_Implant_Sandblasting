{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\iNeuron_Projects\\\\End_to_End_ML_Dental_Implant_Sandblasting\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\iNeuron_Projects\\\\End_to_End_ML_Dental_Implant_Sandblasting'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from Dental_Implant_Sandblasting import logger\n",
    "from Dental_Implant_Sandblasting.utils.common import read_yaml, create_directories\n",
    "from Dental_Implant_Sandblasting.constants import CONFIG_FILE_PATH, PARAMS_FILE_PATH, SCHEMA_FILE_PATH\n",
    "from sklearn.linear_model import Ridge, ElasticNet, BayesianRidge, HuberRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "\n",
    "# Define ModelTrainerConfig dataclass\n",
    "@dataclass(frozen=True)\n",
    "class ModelTrainerConfig:\n",
    "    root_dir: Path\n",
    "    train_data_path: Path\n",
    "    test_data_path: Path\n",
    "    test_size: float\n",
    "    random_state: int\n",
    "    models: dict\n",
    "    param_grids: dict\n",
    "    alpha: float\n",
    "    l1_ratio: float\n",
    "    target_columns: list\n",
    "\n",
    "# Define ConfigurationManager class\n",
    "class ConfigurationManager:\n",
    "    def __init__(self, config_filepath=CONFIG_FILE_PATH, params_filepath=PARAMS_FILE_PATH, schema_filepath=SCHEMA_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_model_trainer_config(self) -> ModelTrainerConfig:\n",
    "        config = self.config.model_trainer\n",
    "        params = self.params.model_training\n",
    "        param_grids = self.params.hyperparameter_tuning\n",
    "        target_columns = list(self.schema.TARGET_COLUMNS.keys())\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "        model_trainer_config = ModelTrainerConfig(\n",
    "            root_dir=Path(config.root_dir),\n",
    "            train_data_path=Path(config.train_data_path),\n",
    "            test_data_path=Path(config.test_data_path),\n",
    "            test_size=params['test_size'],\n",
    "            random_state=params['random_state'],\n",
    "            models=params['models'],\n",
    "            param_grids=param_grids,\n",
    "            alpha=params['elasticnet']['alpha'],\n",
    "            l1_ratio=params['elasticnet']['l1_ratio'],\n",
    "            target_columns=target_columns\n",
    "        )\n",
    "        return model_trainer_config\n",
    "\n",
    "# Define ModelTrainer class\n",
    "class ModelTrainer:\n",
    "    def __init__(self, config: ModelTrainerConfig):\n",
    "        self.config = config\n",
    "        self.models = {\n",
    "            \"Ridge\": Ridge(),\n",
    "            \"ElasticNet\": ElasticNet(alpha=self.config.alpha, l1_ratio=self.config.l1_ratio),\n",
    "            \"BayesianRidge\": BayesianRidge(),\n",
    "            \"HuberRegressor\": HuberRegressor(),\n",
    "            \"RandomForest\": RandomForestRegressor(random_state=self.config.random_state),\n",
    "            \"GradientBoosting\": GradientBoostingRegressor(random_state=self.config.random_state),\n",
    "            \"SVR\": SVR(),\n",
    "            \"XGBRegressor\": XGBRegressor(random_state=self.config.random_state)\n",
    "        }\n",
    "        self.param_grids = self.config.param_grids\n",
    "\n",
    "    def load_data(self):\n",
    "        train_data = pd.read_csv(self.config.train_data_path)\n",
    "        test_data = pd.read_csv(self.config.test_data_path)\n",
    "\n",
    "        X_train = train_data.drop(columns=self.config.target_columns)\n",
    "        y_train = train_data[self.config.target_columns]\n",
    "\n",
    "        X_test = test_data.drop(columns=self.config.target_columns)\n",
    "        y_test = test_data[self.config.target_columns]\n",
    "\n",
    "        return X_train, y_train, X_test, y_test\n",
    "\n",
    "    def evaluate_models(self, X_train, y_train, target_column):\n",
    "        model_performance = {}\n",
    "\n",
    "        for model_name, model in self.models.items():\n",
    "            print(f\"Training {model_name} for {target_column}...\")\n",
    "            cv_scores = cross_val_score(model, X_train, y_train[target_column], cv=5, scoring='neg_mean_absolute_error')\n",
    "            mae = -cv_scores.mean()\n",
    "\n",
    "            if model_name not in model_performance:\n",
    "                model_performance[model_name] = {}\n",
    "\n",
    "            model_performance[model_name][f\"MAE ({target_column})\"] = mae\n",
    "\n",
    "            print(f\"{model_name} - MAE ({target_column}): {mae:.4f}\")\n",
    "\n",
    "        return model_performance\n",
    "\n",
    "    def hyperparameter_tuning(self, X_train, y_train, target_column):\n",
    "        best_models = {}\n",
    "\n",
    "        for model_name in [\"RandomForest\", \"GradientBoosting\", \"Ridge\", \"ElasticNet\", \"HuberRegressor\", \"SVR\", \"XGBRegressor\"]:\n",
    "            grid_search = GridSearchCV(\n",
    "                estimator=self.models[model_name],\n",
    "                param_grid=self.param_grids[model_name.lower()],\n",
    "                cv=5,\n",
    "                scoring='neg_mean_absolute_error',\n",
    "                n_jobs=-1,\n",
    "                verbose=2\n",
    "            )\n",
    "            print(f\"Tuning {model_name} for {target_column}...\")\n",
    "            grid_search.fit(X_train, y_train[target_column])\n",
    "            best_models[model_name] = grid_search.best_estimator_\n",
    "            print(f\"Best parameters for {model_name} ({target_column}): {grid_search.best_params_}\")\n",
    "\n",
    "        return best_models\n",
    "\n",
    "    def evaluate_best_models(self, best_models, X_test, y_test, target_column):\n",
    "        performance_metrics = {}\n",
    "\n",
    "        for model_name, model in best_models.items():\n",
    "            y_pred = model.predict(X_test)\n",
    "            mae = mean_absolute_error(y_test[target_column], y_pred)\n",
    "            rmse = np.sqrt(mean_squared_error(y_test[target_column], y_pred))\n",
    "            r2 = r2_score(y_test[target_column], y_pred)\n",
    "\n",
    "            if model_name not in performance_metrics:\n",
    "                performance_metrics[model_name] = {}\n",
    "\n",
    "            performance_metrics[model_name][f\"MAE ({target_column})\"] = mae\n",
    "            performance_metrics[model_name][f\"RMSE ({target_column})\"] = rmse\n",
    "            performance_metrics[model_name][f\"R2 ({target_column})\"] = r2\n",
    "\n",
    "            print(f\"{model_name} - Test MAE ({target_column}): {mae:.4f}\")\n",
    "            print(f\"{model_name} - Test RMSE ({target_column}): {rmse:.4f}\")\n",
    "            print(f\"{model_name} - Test R2 ({target_column}): {r2:.4f}\")\n",
    "\n",
    "        return performance_metrics\n",
    "\n",
    "    def save_best_models(self, best_models):\n",
    "        for model_name, model in best_models.items():\n",
    "            joblib.dump(model, self.config.root_dir / f\"{model_name}.joblib\")\n",
    "            logger.info(f\"Saved best model {model_name} to {self.config.root_dir / f'{model_name}.joblib'}\")\n",
    "\n",
    "    def execute(self):\n",
    "        X_train, y_train, X_test, y_test = self.load_data()\n",
    "        overall_performance = {}\n",
    "\n",
    "        for target_column in self.config.target_columns:\n",
    "            print(f\"\\nEvaluating models for target column: {target_column}\")\n",
    "            model_performance = self.evaluate_models(X_train, y_train, target_column)\n",
    "            overall_performance[target_column] = model_performance\n",
    "\n",
    "            best_models = self.hyperparameter_tuning(X_train, y_train, target_column)\n",
    "            performance_metrics = self.evaluate_best_models(best_models, X_test, y_test, target_column)\n",
    "\n",
    "            best_hyperparameters = {model_name: model.get_params() for model_name, model in best_models.items()}\n",
    "            print(f\"\\nBest Hyperparameters for {target_column}:\\n\", best_hyperparameters)\n",
    "            print(f\"\\nPerformance Metrics for {target_column}:\\n\", performance_metrics)\n",
    "\n",
    "            self.save_best_models(best_models)\n",
    "\n",
    "        # Visualizations\n",
    "        for target_column in self.config.target_columns:\n",
    "            metrics_df = pd.DataFrame(overall_performance[target_column]).T\n",
    "\n",
    "            plt.figure(figsize=(14, 6))\n",
    "            plt.subplot(1, 3, 1)\n",
    "            sns.barplot(x=metrics_df.index, y=[metrics_df[f'MAE ({target_column})'][model] for model in metrics_df.index])\n",
    "            plt.title(f'MAE for Best Models ({target_column})')\n",
    "            plt.ylabel('Mean Absolute Error')\n",
    "            plt.xlabel('Model')\n",
    "\n",
    "            plt.subplot(1, 3, 2)\n",
    "            sns.barplot(x=metrics_df.index, y=[metrics_df[f'RMSE ({target_column})'][model] for model in metrics_df.index])\n",
    "            plt.title(f'RMSE for Best Models ({target_column})')\n",
    "            plt.ylabel('Root Mean Squared Error')\n",
    "            plt.xlabel('Model')\n",
    "\n",
    "            plt.subplot(1, 3, 3)\n",
    "            sns.barplot(x=metrics_df.index, y=[metrics_df[f'R2 ({target_column})'][model] for model in metrics_df.index])\n",
    "            plt.title(f'R2 for Best Models ({target_column})')\n",
    "            plt.ylabel('R-Squared')\n",
    "            plt.xlabel('Model')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "# Pipeline execution\n",
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_trainer_config = config.get_model_trainer_config()\n",
    "    model_trainer = ModelTrainer(config=model_trainer_config)\n",
    "    model_trainer.execute()\n",
    "except Exception as e:\n",
    "    logger.exception(e)\n",
    "    raise e\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
