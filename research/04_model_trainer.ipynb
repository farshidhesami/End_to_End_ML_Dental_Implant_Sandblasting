{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\iNeuron_Projects\\\\End_to_End_ML_Dental_Implant_Sandblasting\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\iNeuron_Projects\\\\End_to_End_ML_Dental_Implant_Sandblasting'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-10 18:46:20,147: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-07-10 18:46:20,190: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-07-10 18:46:20,203: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2024-07-10 18:46:20,206: INFO: common: created directory at: artifacts]\n",
      "[2024-07-10 18:46:20,209: INFO: common: created directory at: artifacts/model_trainer]\n",
      "Debug: Entire params dictionary:\n",
      "{'data_preprocessing': {'imputation_strategy': 'mean'}, 'feature_engineering': {'polynomial_degree': 2}, 'data_transformation': {'test_size': 0.2, 'random_state': 42, 'polynomial_features_degree': 2, 'scaling_method': 'StandardScaler'}, 'model_training': {'test_size': 0.2, 'random_state': 42, 'models': {'ridge': {'alpha': 0.1}, 'elasticnet': {'alpha': 0.1, 'l1_ratio': 0.1}, 'bayesian_ridge': {}, 'huber_regressor': {'epsilon': 1.1, 'max_iter': 1000}}, 'target_column': 'Result (1=Passed, 0=Failed)'}, 'hyperparameter_tuning': {'cv': 5, 'scoring': 'neg_mean_absolute_error', 'random_forest': {'param_grid': {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20, 30], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4]}}, 'gradient_boosting': {'param_grid': {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 0.2], 'max_depth': [3, 5, 7], 'subsample': [0.8, 0.9, 1.0]}}, 'ridge': {'param_grid': {'alpha': [0.1, 1, 10, 100]}}, 'elasticnet': {'param_grid': {'alpha': [0.1, 1, 10], 'l1_ratio': [0.1, 0.5, 0.9]}}, 'huber': {'param_grid': {'epsilon': [1.1, 1.35, 1.5, 1.75]}}, 'svr': {'param_grid': {'C': [0.1, 1, 10], 'epsilon': [0.01, 0.1, 0.2], 'kernel': ['linear', 'rbf']}}, 'xgboost': {'param_grid': {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 0.2], 'max_depth': [3, 5, 7], 'subsample': [0.8, 0.9, 1.0]}}}}\n",
      "Debug: Keys in params:\n",
      "dict_keys(['data_preprocessing', 'feature_engineering', 'data_transformation', 'model_training', 'hyperparameter_tuning'])\n",
      "Debug: Contents of params['model_training']:\n",
      "{'test_size': 0.2, 'random_state': 42, 'models': {'ridge': {'alpha': 0.1}, 'elasticnet': {'alpha': 0.1, 'l1_ratio': 0.1}, 'bayesian_ridge': {}, 'huber_regressor': {'epsilon': 1.1, 'max_iter': 1000}}, 'target_column': 'Result (1=Passed, 0=Failed)'}\n",
      "Debug: Keys in params['model_training']:\n",
      "dict_keys(['test_size', 'random_state', 'models', 'target_column'])\n",
      "[2024-07-10 18:46:20,242: INFO: 2799381885: Training Ridge...]\n",
      "[2024-07-10 18:46:20,328: INFO: 2799381885: Ridge - MAE: 0.1520]\n",
      "[2024-07-10 18:46:20,330: INFO: 2799381885: Training ElasticNet...]\n",
      "[2024-07-10 18:46:20,407: INFO: 2799381885: ElasticNet - MAE: 0.1314]\n",
      "[2024-07-10 18:46:20,409: INFO: 2799381885: Training BayesianRidge...]\n",
      "[2024-07-10 18:46:20,522: INFO: 2799381885: BayesianRidge - MAE: 0.1092]\n",
      "[2024-07-10 18:46:20,525: INFO: 2799381885: Training HuberRegressor...]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Farshid Hesami\\anaconda3\\envs\\mlProject\\lib\\site-packages\\sklearn\\linear_model\\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\Users\\Farshid Hesami\\anaconda3\\envs\\mlProject\\lib\\site-packages\\sklearn\\linear_model\\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\Users\\Farshid Hesami\\anaconda3\\envs\\mlProject\\lib\\site-packages\\sklearn\\linear_model\\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-10 18:46:20,978: INFO: 2799381885: HuberRegressor - MAE: 0.0661]\n",
      "[2024-07-10 18:46:20,981: INFO: 2799381885: Training RandomForest...]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Farshid Hesami\\anaconda3\\envs\\mlProject\\lib\\site-packages\\sklearn\\linear_model\\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\Users\\Farshid Hesami\\anaconda3\\envs\\mlProject\\lib\\site-packages\\sklearn\\linear_model\\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-10 18:46:23,195: INFO: 2799381885: RandomForest - MAE: 0.0076]\n",
      "[2024-07-10 18:46:23,197: INFO: 2799381885: Training GradientBoosting...]\n",
      "[2024-07-10 18:46:24,394: INFO: 2799381885: GradientBoosting - MAE: 0.0005]\n",
      "[2024-07-10 18:46:24,396: INFO: 2799381885: Training SVR...]\n",
      "[2024-07-10 18:46:24,473: INFO: 2799381885: SVR - MAE: 0.1343]\n",
      "[2024-07-10 18:46:24,475: INFO: 2799381885: Training XGBRegressor...]\n",
      "[2024-07-10 18:46:25,286: INFO: 2799381885: XGBRegressor - MAE: 0.0006]\n",
      "\n",
      "Model Performance:\n",
      "                        MAE\n",
      "Ridge             0.151990\n",
      "ElasticNet        0.131432\n",
      "BayesianRidge     0.109247\n",
      "HuberRegressor    0.066115\n",
      "RandomForest      0.007593\n",
      "GradientBoosting  0.000538\n",
      "SVR               0.134326\n",
      "XGBRegressor      0.000623\n",
      "[2024-07-10 18:46:25,342: INFO: 2799381885: Tuning RandomForest...]\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "[2024-07-10 18:47:59,728: INFO: 2799381885: Best parameters for RandomForest: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 50}]\n",
      "[2024-07-10 18:47:59,732: INFO: 2799381885: Tuning GradientBoosting...]\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "[2024-07-10 18:48:38,293: INFO: 2799381885: Best parameters for GradientBoosting: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'subsample': 1.0}]\n",
      "[2024-07-10 18:48:38,296: INFO: 2799381885: Tuning Ridge...]\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "[2024-07-10 18:48:38,475: INFO: 2799381885: Best parameters for Ridge: {'alpha': 100}]\n",
      "[2024-07-10 18:48:38,477: INFO: 2799381885: Tuning ElasticNet...]\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "[2024-07-10 18:48:38,759: INFO: 2799381885: Best parameters for ElasticNet: {'alpha': 1, 'l1_ratio': 0.5}]\n",
      "[2024-07-10 18:48:38,761: INFO: 2799381885: Tuning HuberRegressor...]\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "[2024-07-10 18:48:39,649: INFO: 2799381885: Best parameters for HuberRegressor: {'epsilon': 1.5}]\n",
      "[2024-07-10 18:48:39,651: INFO: 2799381885: Tuning SVR...]\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Farshid Hesami\\anaconda3\\envs\\mlProject\\lib\\site-packages\\sklearn\\linear_model\\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-10 18:48:58,704: INFO: 2799381885: Best parameters for SVR: {'C': 0.1, 'epsilon': 0.01, 'kernel': 'linear'}]\n",
      "[2024-07-10 18:48:58,705: INFO: 2799381885: Tuning XGBRegressor...]\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "[2024-07-10 18:49:15,700: INFO: 2799381885: Best parameters for XGBRegressor: {'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 50, 'subsample': 1.0}]\n",
      "[2024-07-10 18:49:15,817: INFO: 2799381885: Saved best model RandomForest to artifacts\\model_trainer\\RandomForest.joblib]\n",
      "[2024-07-10 18:49:15,866: INFO: 2799381885: Saved best model GradientBoosting to artifacts\\model_trainer\\GradientBoosting.joblib]\n",
      "[2024-07-10 18:49:15,873: INFO: 2799381885: Saved best model Ridge to artifacts\\model_trainer\\Ridge.joblib]\n",
      "[2024-07-10 18:49:15,879: INFO: 2799381885: Saved best model ElasticNet to artifacts\\model_trainer\\ElasticNet.joblib]\n",
      "[2024-07-10 18:49:15,885: INFO: 2799381885: Saved best model HuberRegressor to artifacts\\model_trainer\\HuberRegressor.joblib]\n",
      "[2024-07-10 18:49:15,896: INFO: 2799381885: Saved best model SVR to artifacts\\model_trainer\\SVR.joblib]\n",
      "[2024-07-10 18:49:15,908: INFO: 2799381885: Saved best model XGBRegressor to artifacts\\model_trainer\\XGBRegressor.joblib]\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from Dental_Implant_Sandblasting import logger\n",
    "from Dental_Implant_Sandblasting.utils.common import read_yaml, create_directories\n",
    "from Dental_Implant_Sandblasting.constants import CONFIG_FILE_PATH, PARAMS_FILE_PATH, SCHEMA_FILE_PATH\n",
    "from sklearn.linear_model import Ridge, ElasticNet, BayesianRidge, HuberRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "import joblib\n",
    "\n",
    "# Define ModelTrainerConfig dataclass\n",
    "@dataclass(frozen=True)\n",
    "class ModelTrainerConfig:\n",
    "    root_dir: Path\n",
    "    train_data_path: Path\n",
    "    test_data_path: Path\n",
    "    test_size: float\n",
    "    random_state: int\n",
    "    models: dict\n",
    "    param_grids: dict\n",
    "    alpha: float\n",
    "    l1_ratio: float\n",
    "    target_column: str\n",
    "\n",
    "# Define ConfigurationManager class\n",
    "class ConfigurationManager:\n",
    "    def __init__(self, config_filepath=CONFIG_FILE_PATH, params_filepath=PARAMS_FILE_PATH, schema_filepath=SCHEMA_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "        create_directories([self.config['artifacts_root']])\n",
    "\n",
    "    def get_model_trainer_config(self) -> ModelTrainerConfig:\n",
    "        config = self.config['model_trainer']\n",
    "        params = self.params['model_training']\n",
    "        param_grids = self.params['hyperparameter_tuning']\n",
    "\n",
    "        create_directories([config['root_dir']])\n",
    "\n",
    "        # Debug print statements\n",
    "        print(\"Debug: Entire params dictionary:\")\n",
    "        print(self.params)\n",
    "        print(\"Debug: Keys in params:\")\n",
    "        print(self.params.keys())\n",
    "        print(\"Debug: Contents of params['model_training']:\")\n",
    "        print(params)\n",
    "        print(\"Debug: Keys in params['model_training']:\")\n",
    "        print(params.keys())\n",
    "\n",
    "        try:\n",
    "            alpha = params['models']['elasticnet']['alpha']\n",
    "            l1_ratio = params['models']['elasticnet']['l1_ratio']\n",
    "        except KeyError as e:\n",
    "            logger.error(f\"KeyError: {e} - Check the params.yaml file for the correct structure.\")\n",
    "            raise\n",
    "\n",
    "        target_column = params['target_column']\n",
    "\n",
    "        model_trainer_config = ModelTrainerConfig(\n",
    "            root_dir=Path(config['root_dir']),\n",
    "            train_data_path=Path(config['train_data_path']),\n",
    "            test_data_path=Path(config['test_data_path']),\n",
    "            test_size=params['test_size'],\n",
    "            random_state=params['random_state'],\n",
    "            models=params['models'],\n",
    "            param_grids=param_grids,\n",
    "            alpha=alpha,\n",
    "            l1_ratio=l1_ratio,\n",
    "            target_column=target_column\n",
    "        )\n",
    "        return model_trainer_config\n",
    "\n",
    "# Define ModelTrainer class\n",
    "class ModelTrainer:\n",
    "    def __init__(self, config: ModelTrainerConfig):\n",
    "        self.config = config\n",
    "        self.models = {\n",
    "            \"Ridge\": Ridge(alpha=self.config.models['ridge']['alpha']),\n",
    "            \"ElasticNet\": ElasticNet(alpha=self.config.alpha, l1_ratio=self.config.l1_ratio),\n",
    "            \"BayesianRidge\": BayesianRidge(),\n",
    "            \"HuberRegressor\": HuberRegressor(epsilon=self.config.models['huber_regressor']['epsilon']),\n",
    "            \"RandomForest\": RandomForestRegressor(random_state=self.config.random_state),\n",
    "            \"GradientBoosting\": GradientBoostingRegressor(random_state=self.config.random_state),\n",
    "            \"SVR\": SVR(),\n",
    "            \"XGBRegressor\": XGBRegressor(random_state=self.config.random_state)\n",
    "        }\n",
    "        self.param_grids = {\n",
    "            'RandomForest': self.config.param_grids['random_forest'],\n",
    "            'GradientBoosting': self.config.param_grids['gradient_boosting'],\n",
    "            'Ridge': self.config.param_grids['ridge'],\n",
    "            'ElasticNet': self.config.param_grids['elasticnet'],\n",
    "            'HuberRegressor': self.config.param_grids['huber'],\n",
    "            'SVR': self.config.param_grids['svr'],\n",
    "            'XGBRegressor': self.config.param_grids['xgboost']\n",
    "        }\n",
    "\n",
    "    def load_data(self):\n",
    "        try:\n",
    "            train_data = pd.read_csv(self.config.train_data_path)\n",
    "            test_data = pd.read_csv(self.config.test_data_path)\n",
    "\n",
    "            X_train = train_data.drop(columns=[self.config.target_column])\n",
    "            y_train = train_data[self.config.target_column]\n",
    "\n",
    "            X_test = test_data.drop(columns=[self.config.target_column])\n",
    "            y_test = test_data[self.config.target_column]\n",
    "\n",
    "            return X_train, y_train, X_test, y_test\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading data: {e}\")\n",
    "            raise\n",
    "\n",
    "    def evaluate_models(self, X_train, y_train):\n",
    "        model_performance = {}\n",
    "\n",
    "        for model_name, model in self.models.items():\n",
    "            logger.info(f\"Training {model_name}...\")\n",
    "            try:\n",
    "                cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_absolute_error')\n",
    "                mae = -cv_scores.mean()\n",
    "\n",
    "                model_performance[model_name] = {\n",
    "                    \"MAE\": mae\n",
    "                }\n",
    "\n",
    "                logger.info(f\"{model_name} - MAE: {mae:.4f}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error training {model_name}: {e}\")\n",
    "\n",
    "        return model_performance\n",
    "\n",
    "    def hyperparameter_tuning(self, X_train, y_train):\n",
    "        best_models = {}\n",
    "\n",
    "        for model_name in self.param_grids.keys():\n",
    "            grid_search = GridSearchCV(\n",
    "                estimator=self.models[model_name],\n",
    "                param_grid=self.param_grids[model_name]['param_grid'],\n",
    "                cv=self.config.param_grids['cv'],\n",
    "                scoring=self.config.param_grids['scoring'],\n",
    "                n_jobs=-1,\n",
    "                verbose=2\n",
    "            )\n",
    "            logger.info(f\"Tuning {model_name}...\")\n",
    "            try:\n",
    "                grid_search.fit(X_train, y_train)\n",
    "                best_models[model_name] = grid_search.best_estimator_\n",
    "                logger.info(f\"Best parameters for {model_name}: {grid_search.best_params_}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error tuning {model_name}: {e}\")\n",
    "\n",
    "        return best_models\n",
    "\n",
    "    def save_best_models(self, best_models):\n",
    "        for model_name, model in best_models.items():\n",
    "            try:\n",
    "                joblib.dump(model, self.config.root_dir / f\"{model_name}.joblib\")\n",
    "                logger.info(f\"Saved best model {model_name} to {self.config.root_dir / f'{model_name}.joblib'}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error saving model {model_name}: {e}\")\n",
    "\n",
    "    def execute(self):\n",
    "        try:\n",
    "            X_train, y_train, X_test, y_test = self.load_data()\n",
    "            model_performance = self.evaluate_models(X_train, y_train)\n",
    "            performance_df = pd.DataFrame(model_performance).T\n",
    "            print(\"\\nModel Performance:\\n\", performance_df)\n",
    "\n",
    "            best_models = self.hyperparameter_tuning(X_train, y_train)\n",
    "\n",
    "            self.save_best_models(best_models)\n",
    "        except Exception as e:\n",
    "            logger.exception(e)\n",
    "            raise e\n",
    "\n",
    "# Pipeline execution\n",
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_trainer_config = config.get_model_trainer_config()\n",
    "    model_trainer = ModelTrainer(config=model_trainer_config)\n",
    "    model_trainer.execute()\n",
    "except Exception as e:\n",
    "    logger.exception(e)\n",
    "    raise e\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
