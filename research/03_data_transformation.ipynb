{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\iNeuron_Projects\\\\End_to_End_ML_Dental_Implant_Sandblasting\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\iNeuron_Projects\\\\End_to_End_ML_Dental_Implant_Sandblasting'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import RobustScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.impute import KNNImputer\n",
    "from xgboost import XGBRegressor\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from Dental_Implant_Sandblasting import logger\n",
    "from Dental_Implant_Sandblasting.utils.common import read_yaml, create_directories\n",
    "from Dental_Implant_Sandblasting.constants import CONFIG_FILE_PATH, PARAMS_FILE_PATH, SCHEMA_FILE_PATH\n",
    "\n",
    "# Data class for Data Transformation configuration\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    transformed_train_dir: Path\n",
    "    transformed_test_dir: Path\n",
    "    test_size: float\n",
    "    random_state: int\n",
    "    polynomial_features_degree: int\n",
    "    scaling_method: str\n",
    "    lasso_max_iter: int\n",
    "    knn_n_neighbors: int\n",
    "\n",
    "# Configuration Manager class for loading configurations\n",
    "class ConfigurationManager:\n",
    "    def __init__(self, config_filepath=CONFIG_FILE_PATH, params_filepath=PARAMS_FILE_PATH, schema_filepath=SCHEMA_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        create_directories([self.config['artifacts_root']])\n",
    "\n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config['data_transformation']\n",
    "        params = self.params['data_transformation']\n",
    "        create_directories([config['root_dir']])\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir=Path(config['root_dir']),\n",
    "            data_path=Path(config['data_path']),\n",
    "            transformed_train_dir=Path(config['transformed_train_path']),\n",
    "            transformed_test_dir=Path(config['transformed_test_path']),\n",
    "            test_size=params['test_size'],\n",
    "            random_state=params['random_state'],\n",
    "            polynomial_features_degree=params['polynomial_features_degree'],\n",
    "            scaling_method=params['scaling_method'],\n",
    "            lasso_max_iter=params['lasso_max_iter'],\n",
    "            knn_n_neighbors=params['knn_n_neighbors']\n",
    "        )\n",
    "        return data_transformation_config\n",
    "\n",
    "# Data Transformation class\n",
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def load_data(self):\n",
    "        data = pd.read_csv(self.config.data_path)\n",
    "        logger.info(f\"Data loaded from {self.config.data_path}\")\n",
    "\n",
    "        # Basic Data Exploration\n",
    "        logger.info(f\"Data Head: \\n{data.head()}\")\n",
    "        logger.info(f\"Data Info: \\n{data.info()}\")\n",
    "        logger.info(f\"Data Description: \\n{data.describe()}\")\n",
    "\n",
    "        return data\n",
    "\n",
    "    def preprocess_data(self, data):\n",
    "        # Convert columns to numeric, forcing any errors to NaN\n",
    "        for col in data.columns:\n",
    "            data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "\n",
    "        # Use KNN Imputer to handle missing values\n",
    "        imputer = KNNImputer(n_neighbors=self.config.knn_n_neighbors)\n",
    "        data_imputed = pd.DataFrame(imputer.fit_transform(data), columns=data.columns)\n",
    "\n",
    "        logger.info(\"Missing values handled using KNN imputation\")\n",
    "        return data_imputed\n",
    "\n",
    "    def feature_engineering(self, data):\n",
    "        # Define feature and target columns\n",
    "        feature_columns = [\n",
    "            'angle_sandblasting',\n",
    "            'pressure_sandblasting_bar',\n",
    "            'temperature_acid_etching',\n",
    "            'time_acid_etching_min',\n",
    "            'voltage_anodizing_v',\n",
    "            'time_anodizing_min'\n",
    "        ]\n",
    "        target_column_sa = 'sa_surface_roughness_micrometer'\n",
    "        target_column_cv = 'cell_viability_percent'\n",
    "\n",
    "        X = data[feature_columns]\n",
    "        y_sa = data[target_column_sa]\n",
    "        y_cv = data[target_column_cv]\n",
    "\n",
    "        # Apply PolynomialFeatures\n",
    "        poly = PolynomialFeatures(degree=self.config.polynomial_features_degree, include_bias=False)\n",
    "        X_poly = poly.fit_transform(X)\n",
    "\n",
    "        # Standardize the features using RobustScaler\n",
    "        scaler = RobustScaler()\n",
    "        X_scaled = scaler.fit_transform(X_poly)\n",
    "\n",
    "        # Apply PCA for dimensionality reduction\n",
    "        pca = PCA(n_components=0.95)\n",
    "        X_pca = pca.fit_transform(X_scaled)\n",
    "        logger.info(f\"Number of components after PCA: {X_pca.shape[1]}\")\n",
    "\n",
    "        # Feature Selection using RFE with Lasso\n",
    "        lasso_model = Lasso(alpha=0.01, max_iter=self.config.lasso_max_iter)\n",
    "\n",
    "        rfe_sa = RFE(lasso_model, n_features_to_select=10)\n",
    "        X_sa_rfe = rfe_sa.fit_transform(X_pca, y_sa)\n",
    "\n",
    "        rfe_cv = RFE(lasso_model, n_features_to_select=10)\n",
    "        X_cv_rfe = rfe_cv.fit_transform(X_pca, y_cv)\n",
    "\n",
    "        logger.info(f\"Number of features selected for Sa after RFE: {X_sa_rfe.shape[1]}\")\n",
    "        logger.info(f\"Number of features selected for CV after RFE: {X_cv_rfe.shape[1]}\")\n",
    "\n",
    "        return X_sa_rfe, X_cv_rfe, y_sa, y_cv\n",
    "\n",
    "    def train_test_splitting(self, X_sa_rfe, X_cv_rfe, y_sa, y_cv):\n",
    "        # Split the data into training and testing sets for Surface Roughness (Sa) and Cell Viability (CV)\n",
    "        X_train_sa, X_test_sa, y_sa_train, y_sa_test = train_test_split(X_sa_rfe, y_sa, test_size=self.config.test_size, random_state=self.config.random_state)\n",
    "        X_train_cv, X_test_cv, y_cv_train, y_cv_test = train_test_split(X_cv_rfe, y_cv, test_size=self.config.test_size, random_state=self.config.random_state)\n",
    "\n",
    "        # Ensure directories exist before saving the files\n",
    "        os.makedirs(self.config.transformed_train_dir, exist_ok=True)\n",
    "        os.makedirs(self.config.transformed_test_dir, exist_ok=True)\n",
    "\n",
    "        # Save the transformed datasets\n",
    "        train_data_sa = pd.DataFrame(X_train_sa)\n",
    "        train_data_cv = pd.DataFrame(X_train_cv)\n",
    "        train_data_sa.to_csv(self.config.transformed_train_dir / 'train_sa.csv', index=False)\n",
    "        train_data_cv.to_csv(self.config.transformed_train_dir / 'train_cv.csv', index=False)\n",
    "        logger.info(f\"Training data saved: Sa - {train_data_sa.shape}, CV - {train_data_cv.shape}\")\n",
    "\n",
    "        test_data_sa = pd.DataFrame(X_test_sa)\n",
    "        test_data_cv = pd.DataFrame(X_test_cv)\n",
    "        test_data_sa.to_csv(self.config.transformed_test_dir / 'test_sa.csv', index=False)\n",
    "        test_data_cv.to_csv(self.config.transformed_test_dir / 'test_cv.csv', index=False)\n",
    "        logger.info(f\"Testing data saved: Sa - {test_data_sa.shape}, CV - {test_data_cv.shape}\")\n",
    "\n",
    "    def execute(self):\n",
    "        try:\n",
    "            data = self.load_data()\n",
    "            preprocessed_data = self.preprocess_data(data)\n",
    "            X_sa_rfe, X_cv_rfe, y_sa, y_cv = self.feature_engineering(preprocessed_data)\n",
    "            self.train_test_splitting(X_sa_rfe, X_cv_rfe, y_sa, y_cv)\n",
    "\n",
    "            # Create status file\n",
    "            with open(self.config.root_dir / \"status.txt\", \"w\") as f:\n",
    "                f.write(\"Transformation status: True\")\n",
    "\n",
    "            logger.info(\"Data transformation and splitting completed successfully.\")\n",
    "        except Exception as e:\n",
    "            # Create status file with failure status\n",
    "            with open(self.config.root_dir / \"status.txt\", \"w\") as f:\n",
    "                f.write(\"Transformation status: False\")\n",
    "\n",
    "            logger.exception(e)\n",
    "            raise e\n",
    "\n",
    "# Pipeline execution\n",
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    data_transformation = DataTransformation(config=data_transformation_config)\n",
    "    data_transformation.execute()\n",
    "except Exception as e:\n",
    "    logger.exception(e)\n",
    "    raise e\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
