# Parameters for data preprocessing
data_preprocessing:
  imputation_strategy: "mean"

# Parameters for feature engineering
feature_engineering:
  polynomial_degree: 2

# Parameters for model training
model_training:
  test_size: 0.2
  random_state: 42
  models:
    ridge:
      alpha: 0.1
    elasticnet:
      alpha: 0.1
      l1_ratio: 0.1
    bayesian_ridge: {}
    huber_regressor:
      epsilon: 1.1

# Parameters for hyperparameter tuning
hyperparameter_tuning:
  cv: 5
  scoring: "neg_mean_absolute_error"
  random_forest:
    param_grid:
      n_estimators: [50, 100, 200]
      max_depth: [None, 10, 20, 30]
      min_samples_split: [2, 5, 10]
      min_samples_leaf: [1, 2, 4]
  gradient_boosting:
    param_grid:
      n_estimators: [50, 100, 200]
      learning_rate: [0.01, 0.1, 0.2]
      max_depth: [3, 5, 7]
      subsample: [0.8, 0.9, 1.0]
  ridge:
    param_grid:
      alpha: [0.1, 1, 10, 100]
  elasticnet:
    param_grid:
      alpha: [0.1, 1, 10]
      l1_ratio: [0.1, 0.5, 0.9]
  huber:
    param_grid:
      epsilon: [1.1, 1.35, 1.5, 1.75]
  svr:
    param_grid:
      C: [0.1, 1, 10]
      epsilon: [0.01, 0.1, 0.2]
      kernel: ['linear', 'rbf']
  xgboost:
    param_grid:
      n_estimators: [50, 100, 200]
      learning_rate: [0.01, 0.1, 0.2]
      max_depth: [3, 5, 7]
      subsample: [0.8, 0.9, 1.0]
