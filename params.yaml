# Parameters for data preprocessing
data_preprocessing:
  imputation_strategy: "mean"

# Parameters for data transformation (03_data_transformation.ipynb)
data_transformation:
  polynomial_features_degree: 2         # Degree for polynomial features
  scaling_method: "StandardScaler"      # Scaling method for feature scaling
  test_size: 0.2                        # Test set size as a fraction of total data
  random_state: 42                      # Random state for reproducibility
  lasso_max_iter: 10000                 # Max iterations for Lasso regression

# Parameters for exploratory data analysis (EDA) in data ingestion (01_data_ingestion.ipynb)
eda:
  cv_threshold: 0.99                    # Quantile threshold for detecting outliers in Cell Viability

# Parameters for data validation (02_data_validation.ipynb)
data_validation:
  sa_lower_bound: 1.5                   # Lower bound for valid Surface Roughness (Sa) values
  sa_upper_bound: 2.5                   # Upper bound for valid Surface Roughness (Sa) values

# Parameters for model training
model_training:
  target_column: 'Result (1=Passed, 0=Failed)'
  models:
    ridge:
      alpha: 100.0
    elasticnet:
      alpha: 0.1
      l1_ratio: 0.5
    bayesian_ridge: {}
    huber_regressor:
      alpha: 0.0001
      epsilon: 1.35
      max_iter: 10000  # Increased max_iter
      tol: 0.000001
      warm_start: False
    random_forest:
      n_estimators: 50
      max_depth: 10
      min_samples_leaf: 1
      min_samples_split: 10
      random_state: 42
    gradient_boosting:
      n_estimators: 50
      learning_rate: 0.2
      max_depth: 7
    svr:
      C: 0.1
      epsilon: 0.01
      kernel: "rbf"
    xgboost:
      objective: 'reg:squarederror'
      n_estimators: 200
      learning_rate: 0.1
      max_depth: 5
      subsample: 1.0

# Parameters for hyperparameter tuning
hyperparameter_tuning:
  cv: 5
  scoring: "neg_mean_absolute_error"
  ridge:
    param_grid:
      alpha: [0.1, 1.0, 10.0, 100.0]
  elasticnet:
    param_grid:
      alpha: [0.1, 1.0, 10.0]
      l1_ratio: [0.1, 0.5, 0.9]
  huber_regressor:
    param_grid:
      alpha: [0.0001, 0.001, 0.01]
      epsilon: [1.35, 1.5, 1.75]
      max_iter: [1000, 2000, 5000, 10000]  # Ensure increased max_iter is also here
      tol: [0.000001, 0.00001]  # Ensure adjusted tolerance is also here
  svr:
    param_grid:
      C: [0.1, 1.0, 10.0]
      epsilon: [0.01, 0.1, 1.0]
      kernel: ["linear", "rbf"]
  random_forest:
    param_grid:
      n_estimators: [50, 100, 200]
      max_depth: [10, 20, null]  # Replace 'None' with null
      min_samples_split: [2, 5, 10]
      min_samples_leaf: [1, 2, 4]
  gradient_boosting:
    param_grid:
      n_estimators: [50, 100, 200]
      learning_rate: [0.01, 0.1, 0.2]
      max_depth: [3, 5, 7]
  xgboost:
    param_grid:
      n_estimators: [50, 100, 200]
      learning_rate: [0.01, 0.1, 0.2]
      max_depth: [3, 5, 7]
